{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPik4K7Z2/M9eYMlrkFSDV9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intelpro/ME455_monodepth/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the runtime type to GPU for using parrellel computing.\n",
        "\n",
        "Click runtime on the top left -> Change runtime type -> Click GPU\n"
      ],
      "metadata": {
        "id": "Eq7xTlQnPFTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from collections import OrderedDict\n",
        "import os"
      ],
      "metadata": {
        "id": "UTizbw_D9e3d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create saved_ckpt folder for saving checkpoint."
      ],
      "metadata": {
        "id": "dZYvhWeLQQxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir saved_ckpt\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX6CrpL0-5Oj",
        "outputId": "4e99bdaf-9420-49cc-9935-5f5241901a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘saved_ckpt’: File exists\n",
            "sample_data  saved_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the encoder.pth and depth.pth checkpoints from the following two links and upload them to the saved_ckpt folder.\n",
        "\n",
        "https://drive.google.com/uc?export=download&id=1VuxaGFc9HvVUTSrlcJ-SAxA_oAQ5XEkP\n",
        "\n",
        "https://drive.google.com/uc?export=download&id=1MpI6P2RvuAbNFkbVduxcEpWOcZqGBhjq\n"
      ],
      "metadata": {
        "id": "5RKBpD1KNsZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define basic network blocks (e.g., conv1x1 or conv3x3 or upsample2d) for setting the basic layer of the monocular depth estimation network.\n"
      ],
      "metadata": {
        "id": "sc_k3JRFQk5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For details of FSE module, please refer to HR-Depth: High Resolution Self-Supervised Monocular Depth Estimation(2021 AAAI, Xiaoyang Lyu et al)\n"
      ],
      "metadata": {
        "id": "zBc_XTg1YNc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1x1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Conv1x1, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "T24DOzBA-Ei4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3x3(nn.Module):\n",
        "    \"\"\"Layer to pad and convolve input\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
        "        super(Conv3x3, self).__init__()\n",
        "        if use_refl:\n",
        "            self.pad = nn.ReflectionPad2d(1)\n",
        "        else:\n",
        "            self.pad = nn.ZeroPad2d(1)\n",
        "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pad(x)\n",
        "        out = self.conv(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FO1j-bPY9lcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Layer to perform a convolution followed by ELU\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = Conv3x3(in_channels, out_channels)\n",
        "        self.nonlin = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.nonlin(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "azrcBNgN9oqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(x):\n",
        "    \"\"\"Upsample input tensor by a factor of 2\n",
        "    \"\"\"\n",
        "    return F.interpolate(x, scale_factor=2, mode=\"nearest\")"
      ],
      "metadata": {
        "id": "FJOlRatT9q2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fSEModule(nn.Module):\n",
        "    def __init__(self, high_feature_channel, low_feature_channels, output_channel=None):\n",
        "        super(fSEModule, self).__init__()\n",
        "        in_channel = high_feature_channel + low_feature_channels\n",
        "        out_channel = high_feature_channel\n",
        "        if output_channel is not None:\n",
        "            out_channel = output_channel\n",
        "        reduction = 16\n",
        "        channel = in_channel\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False)\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.conv_se = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, high_features, low_features):\n",
        "        features = [upsample(high_features)]\n",
        "        features += low_features\n",
        "        features = torch.cat(features, 1)\n",
        "\n",
        "        b, c, _, _ = features.size()\n",
        "        y = self.avg_pool(features).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "\n",
        "        y = self.sigmoid(y)\n",
        "        features = features * y.expand_as(features)\n",
        "\n",
        "        return self.relu(self.conv_se(features))"
      ],
      "metadata": {
        "id": "ENm_WPha9s19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a basic layers to perform the essential functions for networks.\n",
        "After that, we define the ResNet for encoding image information. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wgVzni36Yh7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetMultiImageInput(models.ResNet):\n",
        "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
        "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "u4mQJcQB90BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        num_input_images (int): Number of frames stacked as input\n",
        "    \"\"\"\n",
        "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
        "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
        "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
        "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
        "\n",
        "    if pretrained:\n",
        "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
        "        loaded['conv1.weight'] = torch.cat(\n",
        "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
        "        model.load_state_dict(loaded)\n",
        "    return model"
      ],
      "metadata": {
        "id": "rmfsyg9y92fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetEncoder(nn.Module):\n",
        "    \"\"\"Pytorch module for a resnet encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
        "        super(ResnetEncoder, self).__init__()\n",
        "\n",
        "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
        "\n",
        "        resnets = {18: models.resnet18,\n",
        "                   34: models.resnet34,\n",
        "                   50: models.resnet50,\n",
        "                   101: models.resnet101,\n",
        "                   152: models.resnet152}\n",
        "\n",
        "        if num_layers not in resnets:\n",
        "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
        "\n",
        "        if num_input_images > 1:\n",
        "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
        "        else:\n",
        "            self.encoder = resnets[num_layers](pretrained)\n",
        "\n",
        "        if num_layers > 34:\n",
        "            self.num_ch_enc[1:] *= 4\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        features = []\n",
        "        x = (input_image - 0.45) / 0.225\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        features.append(self.encoder.relu(x))\n",
        "        features.append(self.encoder.layer1(self.encoder.maxpool(features[-1])))\n",
        "        features.append(self.encoder.layer2(features[-1]))\n",
        "        features.append(self.encoder.layer3(features[-1]))\n",
        "        features.append(self.encoder.layer4(features[-1]))\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "mZysoJBe942i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After definining the encoder layer, we also define depth decoder layer to decode the depth information."
      ],
      "metadata": {
        "id": "SkZHMupLZgb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HRDepthDecoder(nn.Module):\n",
        "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, mobile_encoder=False):\n",
        "        super(HRDepthDecoder, self).__init__()\n",
        "\n",
        "        self.num_output_channels = num_output_channels\n",
        "        self.num_ch_enc = num_ch_enc\n",
        "        self.scales = scales\n",
        "        self.mobile_encoder = mobile_encoder\n",
        "        if mobile_encoder:\n",
        "            self.num_ch_dec = np.array([4, 12, 20, 40, 80])\n",
        "        else:\n",
        "            self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
        "\n",
        "        self.all_position = [\"01\", \"11\", \"21\", \"31\", \"02\", \"12\", \"22\", \"03\", \"13\", \"04\"]\n",
        "        self.attention_position = [\"31\", \"22\", \"13\", \"04\"]\n",
        "        self.non_attention_position = [\"01\", \"11\", \"21\", \"02\", \"12\", \"03\"]\n",
        "            \n",
        "        self.convs = nn.ModuleDict()\n",
        "        for j in range(5):\n",
        "            for i in range(5 - j):\n",
        "                # upconv 0\n",
        "                num_ch_in = num_ch_enc[i]\n",
        "                if i == 0 and j != 0:\n",
        "                    num_ch_in /= 2\n",
        "                num_ch_out = num_ch_in / 2\n",
        "                self.convs[\"X_{}{}_Conv_0\".format(i, j)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "                # X_04 upconv 1, only add X_04 convolution\n",
        "                if i == 0 and j == 4:\n",
        "                    num_ch_in = num_ch_out\n",
        "                    num_ch_out = self.num_ch_dec[i]\n",
        "                    self.convs[\"X_{}{}_Conv_1\".format(i, j)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        # declare fSEModule and original module\n",
        "        for index in self.attention_position:\n",
        "            row = int(index[0])\n",
        "            col = int(index[1])\n",
        "            if mobile_encoder:\n",
        "                self.convs[\"X_\" + index + \"_attention\"] = fSEModule(num_ch_enc[row + 1] // 2, self.num_ch_enc[row]\n",
        "                                                                          + self.num_ch_dec[row]*2*(col-1),\n",
        "                                                                         output_channel=self.num_ch_dec[row] * 2)\n",
        "            else:\n",
        "                self.convs[\"X_\" + index + \"_attention\"] = fSEModule(num_ch_enc[row + 1] // 2, self.num_ch_enc[row]\n",
        "                                                                         + self.num_ch_dec[row + 1] * (col - 1))\n",
        "        for index in self.non_attention_position:\n",
        "            row = int(index[0])\n",
        "            col = int(index[1])\n",
        "            if mobile_encoder:\n",
        "                self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(\n",
        "                    self.num_ch_enc[row]+ self.num_ch_enc[row + 1] // 2 +\n",
        "                    self.num_ch_dec[row]*2*(col-1), self.num_ch_dec[row] * 2)\n",
        "            else:\n",
        "                if col == 1:\n",
        "                    self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(num_ch_enc[row + 1] // 2 +\n",
        "                                                                            self.num_ch_enc[row], self.num_ch_dec[row + 1])\n",
        "                else:\n",
        "                    self.convs[\"X_\"+index+\"_downsample\"] = Conv1x1(num_ch_enc[row+1] // 2 + self.num_ch_enc[row]\n",
        "                                                                          + self.num_ch_dec[row+1]*(col-1), self.num_ch_dec[row + 1] * 2)\n",
        "                    self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(self.num_ch_dec[row + 1] * 2, self.num_ch_dec[row + 1])\n",
        "\n",
        "        if self.mobile_encoder:\n",
        "            self.convs[\"dispConvScale0\"] = Conv3x3(4, self.num_output_channels)\n",
        "            self.convs[\"dispConvScale1\"] = Conv3x3(8, self.num_output_channels)\n",
        "            self.convs[\"dispConvScale2\"] = Conv3x3(24, self.num_output_channels)\n",
        "            self.convs[\"dispConvScale3\"] = Conv3x3(40, self.num_output_channels)\n",
        "        else:\n",
        "            for i in range(4):\n",
        "                self.convs[\"dispConvScale{}\".format(i)] = Conv3x3(self.num_ch_dec[i], self.num_output_channels)\n",
        "\n",
        "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def nestConv(self, conv, high_feature, low_features):\n",
        "        conv_0 = conv[0]\n",
        "        conv_1 = conv[1]\n",
        "        assert isinstance(low_features, list)\n",
        "        high_features = [upsample(conv_0(high_feature))]\n",
        "        for feature in low_features:\n",
        "            high_features.append(feature)\n",
        "        high_features = torch.cat(high_features, 1)\n",
        "        if len(conv) == 3:\n",
        "            high_features = conv[2](high_features)\n",
        "        return conv_1(high_features)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        outputs = {}\n",
        "        features = {}\n",
        "        for i in range(5):\n",
        "            features[\"X_{}0\".format(i)] = input_features[i]\n",
        "        # Network architecture\n",
        "        for index in self.all_position:\n",
        "            row = int(index[0])\n",
        "            col = int(index[1])\n",
        "            low_features = []\n",
        "            for i in range(col):\n",
        "                low_features.append(features[\"X_{}{}\".format(row, i)])\n",
        "            # add fSE block to decoder\n",
        "            if index in self.attention_position:\n",
        "                features[\"X_\"+index] = self.convs[\"X_\" + index + \"_attention\"](\n",
        "                    self.convs[\"X_{}{}_Conv_0\".format(row+1, col-1)](features[\"X_{}{}\".format(row+1, col-1)]), low_features)\n",
        "            elif index in self.non_attention_position:\n",
        "                conv = [self.convs[\"X_{}{}_Conv_0\".format(row + 1, col - 1)],\n",
        "                        self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)]]\n",
        "                if col != 1 and not self.mobile_encoder:\n",
        "                    conv.append(self.convs[\"X_\" + index + \"_downsample\"])\n",
        "                features[\"X_\" + index] = self.nestConv(conv, features[\"X_{}{}\".format(row+1, col-1)], low_features)\n",
        "\n",
        "        x = features[\"X_04\"]\n",
        "        x = self.convs[\"X_04_Conv_0\"](x)\n",
        "        x = self.convs[\"X_04_Conv_1\"](upsample(x))\n",
        "        outputs[(\"disparity\", \"Scale0\")] = self.sigmoid(self.convs[\"dispConvScale0\"](x))\n",
        "        outputs[(\"disparity\", \"Scale1\")] = self.sigmoid(self.convs[\"dispConvScale1\"](features[\"X_04\"]))\n",
        "        outputs[(\"disparity\", \"Scale2\")] = self.sigmoid(self.convs[\"dispConvScale2\"](features[\"X_13\"]))\n",
        "        outputs[(\"disparity\", \"Scale3\")] = self.sigmoid(self.convs[\"dispConvScale3\"](features[\"X_22\"]))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Z8P9tmMi-O0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have defined the basic layer for every network. Load the checkpoint and extract the result."
      ],
      "metadata": {
        "id": "12kgHnaJZrk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depth_encoder = ResnetEncoder(18, False)\n",
        "depth_decoder = HRDepthDecoder(depth_encoder.num_ch_enc)\n",
        "\n",
        "depth_encoder_path = \"./saved_ckpt/encoder.pth\"\n",
        "depth_decoder_path = \"./saved_ckpt/depth.pth\"\n",
        "\n",
        "encoder_dict = torch.load(depth_encoder_path)\n",
        "img_height = encoder_dict[\"height\"]\n",
        "img_width = encoder_dict[\"width\"]\n",
        "print(\"Test image height is:\", img_height)\n",
        "print(\"Test image width is:\", img_width)\n",
        "load_dict = {k: v for k, v in encoder_dict.items() if k in depth_encoder.state_dict()}\n",
        "\n",
        "decoder_dict = torch.load(depth_decoder_path)\n",
        "\n",
        "depth_encoder.load_state_dict(load_dict)\n",
        "depth_decoder.load_state_dict(decoder_dict)"
      ],
      "metadata": {
        "id": "ro3m-eew-RFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the check point, the monodepth results corresponding to the images are extracted and saved the results."
      ],
      "metadata": {
        "id": "RbN-afl6I2sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H_org = 1024\n",
        "W_org = 2048\n",
        "\n",
        "## image directory\n",
        "img_dir = '../imgs'\n",
        "img_list = os.listdir(img_dir)\n",
        "img_list.sort()\n",
        "## saving directory \n",
        "saved_disp_dir = None\n",
        "saved_inv_disp_dir = None\n",
        "if not os.path.exists(saved_disp_dir):\n",
        "  os.makedirs(saved_disp_dir)\n",
        "if not os.path.exists(saved_inv_disp_dir):\n",
        "    os.makedirs(saved_inv_disp_dir)\n",
        "for img_name in img_list:\n",
        "    img_path = os.path.join(img_dir, img_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (img_width, img_height))\n",
        "\n",
        "    image_tensor = transforms.ToTensor()(image).unsqueeze(0)\n",
        "\n",
        "    # predict depth from single image\n",
        "    result = depth_decoder(depth_encoder(image_tensor))\n",
        "    disp_est = result[('disparity'), ('Scale0')]\n",
        "    disp_est_np = disp_est.detach().cpu().numpy().squeeze()\n",
        "    _pred_disp = W_org * cv2.resize(disp_est_np, (W_org, H_org), interpolation=cv2.INTER_LINEAR)\n",
        "    ### for saving disparity \n",
        "    np.savez_compressed(os.path.join(saved_disp_dir, img_name.split(\".\")[0] + '.npz'), disp=_pred_disp)\n",
        "    ### for saving inverse-disaprity \n",
        "    inv_disp = 1/_pred_disp\n",
        "    np.savez_compressed(os.path.join(saved_inv_disp_dir, img_name.split(\".\")[0] + '.npz'), inv_disp=inv_disp)"
      ],
      "metadata": {
        "id": "v2Xpzz7paIGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}